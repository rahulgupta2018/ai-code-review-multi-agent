"""
Simple Result Cache for Code Reviews
Caches analysis results to avoid redundant LLM calls for identical code
"""

import hashlib
import json
import time
from pathlib import Path
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class SimpleResultCache:
    """
    File-based cache for code review results.
    Uses content hash to identify duplicate code submissions.
    """
    
    def __init__(self, cache_dir: str = "./cache", ttl_seconds: int = 3600):
        """
        Initialize cache.
        
        Args:
            cache_dir: Directory to store cache files
            ttl_seconds: Time-to-live for cache entries (default: 1 hour)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl_seconds = ttl_seconds
        logger.info(f"âœ… ResultCache initialized: {self.cache_dir} (TTL: {ttl_seconds}s)")
    
    def _get_content_hash(self, code: str, analysis_type: str) -> str:
        """Generate hash from code content and analysis type."""
        content = f"{code}:{analysis_type}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _get_cache_path(self, content_hash: str) -> Path:
        """Get file path for cache entry."""
        return self.cache_dir / f"{content_hash}.json"
    
    def get(self, code: str, analysis_type: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve cached result if available and not expired.
        
        Args:
            code: Source code string
            analysis_type: Type of analysis (e.g., "security", "quality", "full")
        
        Returns:
            Cached result dict or None if not found/expired
        """
        content_hash = self._get_content_hash(code, analysis_type)
        cache_path = self._get_cache_path(content_hash)
        
        if not cache_path.exists():
            logger.debug(f"Cache MISS: {content_hash[:8]}")
            return None
        
        try:
            with open(cache_path, 'r') as f:
                cached_data = json.load(f)
            
            # Check if expired
            cached_time = cached_data.get('timestamp', 0)
            age = time.time() - cached_time
            
            if age > self.ttl_seconds:
                logger.info(f"Cache EXPIRED: {content_hash[:8]} (age: {age:.0f}s)")
                cache_path.unlink()  # Delete expired cache
                return None
            
            logger.info(f"Cache HIT: {content_hash[:8]} (age: {age:.0f}s)")
            return cached_data.get('result')
            
        except Exception as e:
            logger.error(f"Cache read error: {e}")
            return None
    
    def set(self, code: str, analysis_type: str, result: Dict[str, Any]) -> None:
        """
        Store analysis result in cache.
        
        Args:
            code: Source code string
            analysis_type: Type of analysis
            result: Analysis result to cache
        """
        content_hash = self._get_content_hash(code, analysis_type)
        cache_path = self._get_cache_path(content_hash)
        
        try:
            cached_data = {
                'timestamp': time.time(),
                'content_hash': content_hash,
                'analysis_type': analysis_type,
                'result': result
            }
            
            with open(cache_path, 'w') as f:
                json.dump(cached_data, f, indent=2)
            
            logger.info(f"Cache SET: {content_hash[:8]}")
            
        except Exception as e:
            logger.error(f"Cache write error: {e}")
    
    def clear_expired(self) -> int:
        """
        Clear all expired cache entries.
        
        Returns:
            Number of entries deleted
        """
        deleted = 0
        current_time = time.time()
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f)
                
                cached_time = cached_data.get('timestamp', 0)
                if current_time - cached_time > self.ttl_seconds:
                    cache_file.unlink()
                    deleted += 1
                    
            except Exception:
                continue
        
        if deleted > 0:
            logger.info(f"Cleared {deleted} expired cache entries")
        
        return deleted
    
    def clear_all(self) -> int:
        """
        Clear all cache entries.
        
        Returns:
            Number of entries deleted
        """
        deleted = 0
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
            deleted += 1
        
        logger.info(f"Cleared all cache: {deleted} entries")
        return deleted


# Global cache instance
_cache_instance: Optional[SimpleResultCache] = None


def get_cache() -> SimpleResultCache:
    """Get or create global cache instance."""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = SimpleResultCache()
    return _cache_instance
