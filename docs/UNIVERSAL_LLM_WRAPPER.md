# Universal LLM Wrapper Documentation

## Overview

The Universal LLM Wrapper (`util/llm_wrapper.py`) provides a **provider-agnostic** abstraction for rate limiting and error handling across **any LLM provider** supported by Google ADK.

## Supported Providers

### ‚úÖ Currently Supported

| Provider | Model Examples | Rate Limit | Local/Remote |
|----------|----------------|------------|--------------|
| **Google Gemini** | gemini-2.5-flash, gemini-2.0-flash, gemini-1.5-pro | 15 RPM (free), 360 RPM (paid) | Remote |
| **Ollama** | granite4:latest, gemma3:latest, llama3, mistral | 30+ RPM (hardware dependent) | Local |
| **OpenAI** | gpt-4, gpt-4-turbo, gpt-3.5-turbo | 20-60 RPM (tier dependent) | Remote |
| **Anthropic** | claude-3-opus, claude-3-sonnet | 20-50 RPM (tier dependent) | Remote |

### üîÑ Any ADK-Compatible Provider

The wrapper works with **any model** that implements ADK's standard interface:
```python
async def generate_content_async(self, *args, **kwargs) -> Any:
    """Standard ADK interface"""
```

## Architecture

### Component Structure

```
util/
‚îú‚îÄ‚îÄ llm_wrapper.py           # Generic wrapper for all providers
‚îú‚îÄ‚îÄ rate_limiter.py          # Token bucket algorithm
‚îî‚îÄ‚îÄ llm_model.py             # Model configuration (existing)

orchestrator_agent/
‚îî‚îÄ‚îÄ agent.py                 # Sequential execution (rate limit protection)
```

### How It Works

```
User Request
    ‚Üì
Orchestrator Agent
    ‚Üì
Sequential Agent Execution (2s delays)
    ‚Üì
    ‚îú‚îÄ‚Üí Agent 1 ‚Üí [Rate Limiter] ‚Üí LLM Provider (Gemini/Ollama/etc.)
    ‚îú‚îÄ‚Üí Agent 2 ‚Üí [Rate Limiter] ‚Üí LLM Provider
    ‚îú‚îÄ‚Üí Agent 3 ‚Üí [Rate Limiter] ‚Üí LLM Provider
    ‚îî‚îÄ‚Üí Agent 4 ‚Üí [Rate Limiter] ‚Üí LLM Provider
    ‚Üì
Report Synthesis
    ‚Üì
Response to User
```

## Configuration

### Provider-Specific Configs

The wrapper includes optimized configurations for each provider:

```python
from util.llm_wrapper import get_provider_config

# Gemini Free Tier (conservative)
gemini_config = get_provider_config('gemini')
# ‚Üí requests_per_minute=10, burst_size=3, cooldown=30s

# Gemini Paid Tier (Standard/Pro)
gemini_paid_config = get_provider_config('gemini_paid')
# ‚Üí requests_per_minute=60, burst_size=10, cooldown=10s

# Ollama (local, more generous)
ollama_config = get_provider_config('ollama')
# ‚Üí requests_per_minute=30, burst_size=5, cooldown=5s

# OpenAI (Tier 1)
openai_config = get_provider_config('openai')
# ‚Üí requests_per_minute=20, burst_size=5, cooldown=20s
```

### Custom Configuration

```python
from util.rate_limiter import RateLimitConfig
from util.llm_wrapper import UniversalLLMWrapper

# Define custom limits
custom_config = RateLimitConfig(
    requests_per_minute=15,      # 15 requests per minute
    burst_size=4,                # Allow 4 rapid requests
    cooldown_on_error=20.0       # 20s cooldown after errors
)

# Create wrapper
wrapper = UniversalLLMWrapper(
    rate_limit_config=custom_config,
    provider_name="CustomProvider"
)
```

## Usage Examples

### Example 1: Switching Between Gemini and Ollama

**Current Setup (in `.env`):**
```bash
# Use Gemini
GEMINI_MODEL=gemini-2.5-flash
GOOGLE_API_KEY=your_key_here

# Or use Ollama (comment out Gemini, update llm_model.py)
OLLAMA_API_BASE=http://localhost:11434
OLLAMA_MODEL=ollama_chat/granite4:latest
OLLAMA_SUBAGENT_MODEL=ollama_chat/gemma3:latest
```

**In `util/llm_model.py`:**
```python
# Switch between providers by commenting/uncommenting:

# Option 1: Use Gemini (current)
agent_model = Gemini(model=GEMINI_MODEL)
sub_agent_model = Gemini(model=GEMINI_MODEL)

# Option 2: Use Ollama (uncomment to switch)
# agent_model = LiteLlm(model=OLLAMA_MODEL, endpoint=OLLAMA_ENDPOINT, ...)
# sub_agent_model = LiteLlm(model=OLLAMA_SUBAGENT_MODEL, endpoint=OLLAMA_ENDPOINT, ...)
```

**Rate limiting automatically adapts:**
- Gemini: 10 RPM, 2s delays between agents ‚úÖ
- Ollama: 30 RPM, 2s delays between agents ‚úÖ (can be reduced to 1s)

### Example 2: Manual Wrapper Usage (Advanced)

```python
from util.llm_wrapper import UniversalLLMWrapper, get_provider_config
from util.llm_model import get_agent_model

# Get configured model
llm = get_agent_model()

# Create wrapper
config = get_provider_config('gemini')
wrapper = UniversalLLMWrapper(rate_limit_config=config, provider_name="Gemini")

# Use wrapper for individual calls (future enhancement)
response = await wrapper.generate_with_rate_limit(
    llm,
    prompt="Analyze this code...",
    stream=False
)
```

### Example 3: Automatic Provider Detection

```python
import os
from util.llm_wrapper import get_provider_config, UniversalLLMWrapper

# Auto-detect from environment
if "GEMINI_MODEL" in os.environ:
    provider = "gemini"
elif "OLLAMA_MODEL" in os.environ:
    provider = "ollama"
elif "OPENAI_API_KEY" in os.environ:
    provider = "openai"
else:
    provider = "unknown"

# Get appropriate config
config = get_provider_config(provider)
wrapper = UniversalLLMWrapper(rate_limit_config=config, provider_name=provider.title())

print(f"‚úÖ Configured for {provider.title()} with {config.requests_per_minute} RPM")
```

## Error Handling

### Generic Error Detection

The wrapper detects common error patterns across all providers:

```python
# Rate Limit Errors (429)
- "429"
- "rate limit"
- "resource_exhausted"
- "quota"
- "too many requests"

# Availability Errors (503)
- "503"
- "unavailable"
- "overloaded"
- "service unavailable"
- "timeout"

# Server Errors (500)
- "500"
- "internal error"
- "server error"
```

### Provider-Specific Error Examples

**Gemini:**
```
503 UNAVAILABLE: The model is overloaded. Please try again later.
429 RESOURCE_EXHAUSTED: Quota exceeded for quota metric
```

**OpenAI:**
```
429 rate_limit_exceeded: You exceeded your current quota
503 service_unavailable: The server is temporarily unable to handle the request
```

**Ollama:**
```
ConnectionError: Failed to connect to localhost:11434
TimeoutError: Request timed out after 30s
```

All mapped to standard HTTP status codes for consistent handling.

## Performance Characteristics

### Gemini (Remote)

| Metric | Free Tier | Paid Tier |
|--------|-----------|-----------|
| Latency | 1-3s per request | 0.5-2s per request |
| Throughput | ~10 requests/min | ~60 requests/min |
| 4-Agent Review | ~12-15s total | ~8-10s total |
| Cost | Free | $0.01-0.05 per review |

### Ollama (Local)

| Metric | CPU (8-core) | GPU (8GB VRAM) |
|--------|--------------|----------------|
| Latency | 5-10s per request | 1-3s per request |
| Throughput | ~10 requests/min | ~30 requests/min |
| 4-Agent Review | ~20-30s total | ~10-15s total |
| Cost | Hardware only | Hardware only |

### Trade-offs

**Sequential vs Parallel Execution:**

```
Parallel (Before Fix):
‚úó 4 agents start simultaneously
‚úó 6+ requests in 2 seconds
‚úó 503 errors, failed reviews
‚úì Fast when working (~3-5s)

Sequential (Current):
‚úì 1 agent at a time
‚úì 2s delays between agents
‚úì Reliable, no 503 errors
‚úó Slower (~12-15s total)
```

**Recommendation:** Sequential is MUCH better
- 100% success rate vs 50% failure rate
- Predictable performance
- Works with any provider
- Simple to maintain

## Migration Guide

### Switching Providers

#### From Gemini to Ollama

1. **Install Ollama:**
   ```bash
   # macOS
   brew install ollama
   
   # Start Ollama service
   ollama serve
   ```

2. **Pull models:**
   ```bash
   ollama pull granite4:latest
   ollama pull gemma3:latest
   ```

3. **Update `.env`:**
   ```bash
   # Comment out Gemini
   # GOOGLE_API_KEY=...
   # GEMINI_MODEL=gemini-2.5-flash
   
   # Enable Ollama
   OLLAMA_API_BASE=http://localhost:11434
   OLLAMA_MODEL=ollama_chat/granite4:latest
   OLLAMA_SUBAGENT_MODEL=ollama_chat/gemma3:latest
   ```

4. **Update `util/llm_model.py`:**
   ```python
   # Comment out Gemini
   # agent_model = Gemini(model=GEMINI_MODEL)
   
   # Uncomment Ollama
   agent_model = LiteLlm(
       model=OLLAMA_MODEL,
       endpoint=OLLAMA_ENDPOINT,
       temperature=0.4,
       max_tokens=2048
   )
   ```

5. **Restart server:**
   ```bash
   pkill -f "adk web"
   adk web --host 0.0.0.0 --port 8800 .
   ```

#### From Ollama to OpenAI

1. **Get OpenAI API key** from https://platform.openai.com

2. **Update `.env`:**
   ```bash
   OPENAI_API_KEY=sk-...your-key...
   OPENAI_MODEL=gpt-4-turbo
   ```

3. **Update `util/llm_model.py`:**
   ```python
   from google.adk.models.lite_llm import LiteLlm
   
   agent_model = LiteLlm(
       model="gpt-4-turbo",
       api_key=os.getenv("OPENAI_API_KEY"),
       temperature=0.4,
       max_tokens=2048
   )
   ```

4. **Adjust rate limits** (OpenAI Tier 1):
   ```python
   # In orchestrator_agent/agent.py
   delay = 3.0  # 3s delay for OpenAI free tier
   ```

## Monitoring

### Rate Limiter Metrics

```bash
# Watch rate limit events
tail -f adk_web.log | grep -E "Rate limiter|Token acquired|cooldown"

# Example output:
# üö¶ Rate limiter: Token acquired for Gemini, making API call
# ‚è±Ô∏è  Waiting 2.0s before next agent (rate limit protection)...
# ‚è∏Ô∏è  Rate limiter entering cooldown due to 503 error
```

### Provider Performance

```bash
# Track API call timing
tail -f adk_web.log | grep "Sending out request"

# Track agent execution
tail -f adk_web.log | grep "Starting agent"

# Monitor errors
tail -f adk_web.log | grep -E "429|503|ERROR"
```

## Future Enhancements

### Phase 1 (Current) ‚úÖ
- [x] Sequential execution with fixed delays
- [x] Generic wrapper supporting all providers
- [x] Provider-specific configurations
- [x] Basic error detection and logging

### Phase 2 (Next Month)
- [ ] Adaptive rate limiting (speed up when API responsive)
- [ ] Request queue for multiple users
- [ ] Metrics dashboard (Prometheus + Grafana)
- [ ] Circuit breaker pattern

### Phase 3 (Production)
- [ ] Distributed rate limiting (Redis)
- [ ] Multiple API keys with round-robin
- [ ] Auto-scaling based on queue depth
- [ ] Provider failover (Gemini ‚Üí Ollama fallback)

## Troubleshooting

### Issue: 503 Errors Still Occurring

**Symptoms:** Still getting 503 UNAVAILABLE despite sequential execution

**Possible Causes:**
1. Delay too short (increase from 2s to 3s or 5s)
2. Too many concurrent users (implement request queue)
3. Provider having global outage (check status page)

**Solution:**
```python
# In orchestrator_agent/agent.py, increase delay
delay = 5.0  # Increase from 2s to 5s
await asyncio.sleep(delay)
```

### Issue: Ollama Not Responding

**Symptoms:** Connection refused or timeout errors

**Solution:**
```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama if not running
ollama serve

# Test Ollama directly
curl http://localhost:11434/api/generate -d '{
  "model": "granite4:latest",
  "prompt": "Hello"
}'
```

### Issue: OpenAI Rate Limits

**Symptoms:** 429 errors from OpenAI

**Solution:**
```python
# Increase delay for OpenAI free tier
delay = 4.0  # OpenAI Tier 1 = ~15 RPM
```

## Best Practices

1. **Always test locally with Ollama first** - Free, fast iteration
2. **Use Gemini for demos** - Better quality, but watch rate limits
3. **Use OpenAI for production** - Best reliability and support
4. **Monitor metrics** - Set up alerts for 429/503 errors
5. **Implement caching** - Already done! See `util/result_cache.py`
6. **Scale gradually** - Start with free tier, upgrade when needed

## API Key Security

**DO NOT commit API keys to git!**

```bash
# Good ‚úÖ
GOOGLE_API_KEY=sk-...  # In .env (gitignored)

# Bad ‚ùå
GOOGLE_API_KEY="sk-..." # Hardcoded in code
```

**Production deployment:**
```bash
# Use environment variables
export GOOGLE_API_KEY="..."

# Or secrets management
aws secretsmanager get-secret-value --secret-id gemini-api-key
```
