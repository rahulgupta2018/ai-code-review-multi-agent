# LLM Provider Configuration
# Supported providers and their settings

providers:
  ollama:
    type: "local"
    base_url: ${OLLAMA_BASE_URL}
    models:
      - name: "llama3.1:8b"
        context_length: 128000
        temperature_range: [0.0, 1.0]
        recommended_temp: 0.1
        capabilities: ["text_generation", "code_analysis", "reasoning", "function_calling", "embedding", "translation", "long_context", "instruction_following", "tool_use", "retrieval", "safety"]
      - name: "codellama:7b"
        context_length: 100000
        temperature_range: [0.0, 1.0]
        recommended_temp: 0.2
        capabilities: ["code_analysis", "code_generation"]
      - name: "mistral:7b"
        context_length: 32000
        temperature_range: [0.0, 1.0]
        recommended_temp: 0.1
        capabilities: ["text_generation", "reasoning"]
    
    default_settings:
      temperature: 0.1
      max_tokens: 2000
      timeout: 30  # Much faster with native Ollama + Apple M1 Pro GPU
      retry_attempts: 2
      stream: false
    
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 50000

  openai:
    type: "cloud"
    base_url: ${OPENAI_BASE_URL}
    api_key: ${OPENAI_API_KEY}
    models:
      - name: "gpt-4"
        context_length: 128000
        temperature_range: [0.0, 2.0]
        recommended_temp: 0.1
        capabilities: [
          "text_generation",
          "code_analysis",
          "code_generation",
          "reasoning",
          "function_calling",
          "embedding",
          "multimodal_input",
          "long_context",
          "instruction_following",
          "translation",
          "summarization",
          "extraction",
          "classification",
          "tool_use",
          "retrieval",
          "safety",
          "style_adaptation"
        ]
      - name: "gpt-4-turbo"
        context_length: 128000
        temperature_range: [0.0, 2.0]
        recommended_temp: 0.1
        capabilities: ["text_generation", "code_analysis", "reasoning", "function_calling"]
      - name: "gpt-3.5-turbo"
        context_length: 16000
        temperature_range: [0.0, 2.0]
        recommended_temp: 0.1
        capabilities: ["text_generation", "code_analysis", "reasoning", "function_calling"]
    
    default_settings:
      temperature: 0.1
      max_tokens: 2000
      timeout: 30
      retry_attempts: 3
      stream: false
    
    rate_limits:
      requests_per_minute: 500
      tokens_per_minute: 150000

  gemini:
    type: "cloud"
    base_url: ${GEMINI_BASE_URL}
    api_key: ${GEMINI_API_KEY}
    models:
      - name: "gemini-1.5-pro"
        context_length: 1000000
        temperature_range: [0.0, 1.0]
        recommended_temp: 0.1
        capabilities: [
            "text_generation",
            "code_analysis",
            "code_generation",
            "reasoning",
            "function_calling",
            "multimodal_input",
            "long_context",
            "translation",
            "summarization",
            "extraction",
            "classification",
            "structured_output",
            "context_caching",
            "search_grounding",
            "code_execution"
        ]
      - name: "gemini-1.5-flash"
        context_length: 1000000
        temperature_range: [0.0, 1.0]
        recommended_temp: 0.1
        capabilities: ["text_generation", "code_analysis", "reasoning"]
    
    default_settings:
      temperature: 0.1
      max_tokens: 2000
      timeout: 30
      retry_attempts: 3
    
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 32000

# Default provider (no fallback)
default_provider: ${LLM_PROVIDER}

# Model selection for different tasks
# Preferred model will be determined by provider-specific environment variables
task_model_mapping:
  code_analysis:
    temperature: 0.1
    max_tokens: 3000
  
  orchestration:
    temperature: 0.1
    max_tokens: 2000
  
  decision_making:
    temperature: 0.05
    max_tokens: 1500
  
  reasoning:
    temperature: 0.2
    max_tokens: 4000

# Quality settings
quality:
  confidence_threshold: 0.7
  enable_response_validation: true
  enable_content_filtering: true
  max_response_time: 60
  enable_fallback: false  # No fallback - use only selected provider
  
# Monitoring and analytics
monitoring:
  track_usage: true
  track_performance: true
  track_quality_metrics: true
  log_requests: true
  log_responses: false  # Set to true for debugging
  enable_metrics_export: true
